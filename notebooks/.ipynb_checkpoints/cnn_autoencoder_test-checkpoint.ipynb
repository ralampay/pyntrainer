{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import sys \n",
    "import argparse\n",
    "import os\n",
    "import math\n",
    "sys.path.append(os.path.join(os.path.abspath(''), '.'))\n",
    "sys.path.append(os.path.join(os.path.abspath(''), '..'))\n",
    "from pyntrainer.lib.cnn_autoencoder import CnnAutoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1054"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Default parameters\n",
    "img_width = 100\n",
    "img_height = 100\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        #print(filename)\n",
    "        img = cv2.imread(os.path.join(folder,filename))\n",
    "        img = cv2.resize(img, (img_width, img_height))\n",
    "        img = img / 255\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "    return images\n",
    "\n",
    "images = load_images_from_folder(\"/home/ralampay/Desktop/train\")\n",
    "\n",
    "# Perform a transpose against numpy to be valid shape for tensor\n",
    "\n",
    "x = []\n",
    "\n",
    "for img in images:\n",
    "    img_tensor = torch.tensor(img.transpose((2,0,1))).float()\n",
    "    d = img_tensor.detach().numpy()\n",
    "    x.append(d)\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CnnAutoencoder(\n",
      "  (convolutional_layers): ModuleList(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (deconvolutional_layers): ModuleList(\n",
      "    (0): ConvTranspose2d(8, 16, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (1): ConvTranspose2d(16, 3, kernel_size=(2, 2), stride=(2, 2))\n",
      "  )\n",
      "  (criterion): BCELoss()\n",
      ")\n",
      "=> Epoch: 1\tLoss: 0.62073\n",
      "=> Epoch: 2\tLoss: 0.19580\n",
      "=> Epoch: 3\tLoss: 0.13876\n",
      "=> Epoch: 4\tLoss: 0.12846\n",
      "=> Epoch: 5\tLoss: 0.12342\n",
      "=> Epoch: 6\tLoss: 0.12171\n",
      "=> Epoch: 7\tLoss: 0.11921\n",
      "=> Epoch: 8\tLoss: 0.11990\n",
      "=> Epoch: 9\tLoss: 0.12042\n",
      "=> Epoch: 10\tLoss: 0.11871\n",
      "=> Epoch: 11\tLoss: 0.11797\n",
      "=> Epoch: 12\tLoss: 0.11971\n",
      "=> Epoch: 13\tLoss: 0.11895\n",
      "=> Epoch: 14\tLoss: 0.11954\n",
      "=> Epoch: 15\tLoss: 0.11877\n",
      "=> Epoch: 16\tLoss: 0.11949\n",
      "=> Epoch: 17\tLoss: 0.11801\n",
      "=> Epoch: 18\tLoss: 0.11864\n",
      "=> Epoch: 19\tLoss: 0.12044\n",
      "=> Epoch: 20\tLoss: 0.11923\n",
      "=> Epoch: 21\tLoss: 0.11926\n",
      "=> Epoch: 22\tLoss: 0.11767\n",
      "=> Epoch: 23\tLoss: 0.11835\n",
      "=> Epoch: 24\tLoss: 0.11839\n",
      "=> Epoch: 25\tLoss: 0.11913\n",
      "=> Epoch: 26\tLoss: 0.11896\n",
      "=> Epoch: 27\tLoss: 0.11896\n",
      "=> Epoch: 28\tLoss: 0.11838\n",
      "=> Epoch: 29\tLoss: 0.11900\n",
      "=> Epoch: 30\tLoss: 0.11839\n",
      "=> Epoch: 31\tLoss: 0.11903\n",
      "=> Epoch: 32\tLoss: 0.11817\n",
      "=> Epoch: 33\tLoss: 0.11900\n",
      "=> Epoch: 34\tLoss: 0.11813\n",
      "=> Epoch: 35\tLoss: 0.11821\n",
      "=> Epoch: 36\tLoss: 0.11811\n",
      "=> Epoch: 37\tLoss: 0.11896\n",
      "=> Epoch: 38\tLoss: 0.11885\n",
      "=> Epoch: 39\tLoss: 0.11819\n",
      "=> Epoch: 40\tLoss: 0.11977\n",
      "=> Epoch: 41\tLoss: 0.11883\n",
      "=> Epoch: 42\tLoss: 0.11881\n",
      "=> Epoch: 43\tLoss: 0.11814\n",
      "=> Epoch: 44\tLoss: 0.11977\n",
      "=> Epoch: 45\tLoss: 0.11806\n",
      "=> Epoch: 46\tLoss: 0.11805\n",
      "=> Epoch: 47\tLoss: 0.11892\n",
      "=> Epoch: 48\tLoss: 0.11797\n",
      "=> Epoch: 49\tLoss: 0.11818\n",
      "=> Epoch: 50\tLoss: 0.11879\n",
      "=> Epoch: 51\tLoss: 0.11799\n",
      "=> Epoch: 52\tLoss: 0.11733\n",
      "=> Epoch: 53\tLoss: 0.11666\n",
      "=> Epoch: 54\tLoss: 0.11807\n",
      "=> Epoch: 55\tLoss: 0.11978\n",
      "=> Epoch: 56\tLoss: 0.11877\n",
      "=> Epoch: 57\tLoss: 0.11807\n",
      "=> Epoch: 58\tLoss: 0.11963\n",
      "=> Epoch: 59\tLoss: 0.11878\n",
      "=> Epoch: 60\tLoss: 0.11809\n",
      "=> Epoch: 61\tLoss: 0.11817\n",
      "=> Epoch: 62\tLoss: 0.11808\n",
      "=> Epoch: 63\tLoss: 0.11804\n",
      "=> Epoch: 64\tLoss: 0.11795\n",
      "=> Epoch: 65\tLoss: 0.11878\n",
      "=> Epoch: 66\tLoss: 0.12069\n",
      "=> Epoch: 67\tLoss: 0.11877\n",
      "=> Epoch: 68\tLoss: 0.11984\n",
      "=> Epoch: 69\tLoss: 0.11805\n",
      "=> Epoch: 70\tLoss: 0.11878\n",
      "=> Epoch: 71\tLoss: 0.11882\n",
      "=> Epoch: 72\tLoss: 0.11874\n",
      "=> Epoch: 73\tLoss: 0.11723\n",
      "=> Epoch: 74\tLoss: 0.11718\n",
      "=> Epoch: 75\tLoss: 0.11804\n",
      "=> Epoch: 76\tLoss: 0.11874\n",
      "=> Epoch: 77\tLoss: 0.11802\n",
      "=> Epoch: 78\tLoss: 0.11869\n",
      "=> Epoch: 79\tLoss: 0.11968\n",
      "=> Epoch: 80\tLoss: 0.11973\n",
      "=> Epoch: 81\tLoss: 0.11869\n",
      "=> Epoch: 82\tLoss: 0.11884\n",
      "=> Epoch: 83\tLoss: 0.11807\n",
      "=> Epoch: 84\tLoss: 0.11864\n",
      "=> Epoch: 85\tLoss: 0.11867\n",
      "=> Epoch: 86\tLoss: 0.11796\n",
      "=> Epoch: 87\tLoss: 0.11807\n",
      "=> Epoch: 88\tLoss: 0.11796\n",
      "=> Epoch: 89\tLoss: 0.11873\n",
      "=> Epoch: 90\tLoss: 0.11809\n",
      "=> Epoch: 91\tLoss: 0.11808\n",
      "=> Epoch: 92\tLoss: 0.11800\n",
      "=> Epoch: 93\tLoss: 0.11878\n",
      "=> Epoch: 94\tLoss: 0.11800\n",
      "=> Epoch: 95\tLoss: 0.11797\n",
      "=> Epoch: 96\tLoss: 0.11788\n",
      "=> Epoch: 97\tLoss: 0.11870\n",
      "=> Epoch: 98\tLoss: 0.11864\n",
      "=> Epoch: 99\tLoss: 0.11858\n",
      "=> Epoch: 100\tLoss: 0.11719\n",
      "=> Epoch: 101\tLoss: 0.11942\n",
      "=> Epoch: 102\tLoss: 0.11881\n",
      "=> Epoch: 103\tLoss: 0.11797\n",
      "=> Epoch: 104\tLoss: 0.11868\n",
      "=> Epoch: 105\tLoss: 0.11856\n",
      "=> Epoch: 106\tLoss: 0.11856\n",
      "=> Epoch: 107\tLoss: 0.11863\n",
      "=> Epoch: 108\tLoss: 0.11795\n",
      "=> Epoch: 109\tLoss: 0.11885\n",
      "=> Epoch: 110\tLoss: 0.11868\n",
      "=> Epoch: 111\tLoss: 0.11795\n",
      "=> Epoch: 112\tLoss: 0.11795\n",
      "=> Epoch: 113\tLoss: 0.11787\n",
      "=> Epoch: 114\tLoss: 0.11872\n",
      "=> Epoch: 115\tLoss: 0.11809\n",
      "=> Epoch: 116\tLoss: 0.11801\n",
      "=> Epoch: 117\tLoss: 0.11854\n",
      "=> Epoch: 118\tLoss: 0.11859\n",
      "=> Epoch: 119\tLoss: 0.11863\n",
      "=> Epoch: 120\tLoss: 0.11787\n",
      "=> Epoch: 121\tLoss: 0.11862\n",
      "=> Epoch: 122\tLoss: 0.11722\n",
      "=> Epoch: 123\tLoss: 0.11788\n",
      "=> Epoch: 124\tLoss: 0.11864\n",
      "=> Epoch: 125\tLoss: 0.11793\n",
      "=> Epoch: 126\tLoss: 0.11718\n",
      "=> Epoch: 127\tLoss: 0.11797\n",
      "=> Epoch: 128\tLoss: 0.11867\n",
      "=> Epoch: 129\tLoss: 0.11865\n",
      "=> Epoch: 130\tLoss: 0.11969\n",
      "=> Epoch: 131\tLoss: 0.11862\n",
      "=> Epoch: 132\tLoss: 0.11795\n",
      "=> Epoch: 133\tLoss: 0.11850\n",
      "=> Epoch: 134\tLoss: 0.11816\n",
      "=> Epoch: 135\tLoss: 0.11855\n",
      "=> Epoch: 136\tLoss: 0.11712\n",
      "=> Epoch: 137\tLoss: 0.11785\n",
      "=> Epoch: 138\tLoss: 0.11711\n",
      "=> Epoch: 139\tLoss: 0.11974\n",
      "=> Epoch: 140\tLoss: 0.11860\n",
      "=> Epoch: 141\tLoss: 0.11857\n",
      "=> Epoch: 142\tLoss: 0.11788\n",
      "=> Epoch: 143\tLoss: 0.11701\n",
      "=> Epoch: 144\tLoss: 0.11719\n",
      "=> Epoch: 145\tLoss: 0.11717\n",
      "=> Epoch: 146\tLoss: 0.11865\n",
      "=> Epoch: 147\tLoss: 0.11859\n",
      "=> Epoch: 148\tLoss: 0.11855\n",
      "=> Epoch: 149\tLoss: 0.11863\n",
      "=> Epoch: 150\tLoss: 0.11711\n",
      "=> Epoch: 151\tLoss: 0.11852\n",
      "=> Epoch: 152\tLoss: 0.11776\n",
      "=> Epoch: 153\tLoss: 0.11864\n",
      "=> Epoch: 154\tLoss: 0.11790\n",
      "=> Epoch: 155\tLoss: 0.11785\n",
      "=> Epoch: 156\tLoss: 0.11782\n",
      "=> Epoch: 157\tLoss: 0.11788\n",
      "=> Epoch: 158\tLoss: 0.11640\n",
      "=> Epoch: 159\tLoss: 0.11779\n",
      "=> Epoch: 160\tLoss: 0.11783\n",
      "=> Epoch: 161\tLoss: 0.11798\n",
      "=> Epoch: 162\tLoss: 0.11772\n",
      "=> Epoch: 163\tLoss: 0.11726\n",
      "=> Epoch: 164\tLoss: 0.11869\n",
      "=> Epoch: 165\tLoss: 0.11788\n",
      "=> Epoch: 166\tLoss: 0.11960\n",
      "=> Epoch: 167\tLoss: 0.11853\n",
      "=> Epoch: 168\tLoss: 0.11862\n",
      "=> Epoch: 169\tLoss: 0.11727\n",
      "=> Epoch: 170\tLoss: 0.11861\n",
      "=> Epoch: 171\tLoss: 0.11851\n",
      "=> Epoch: 172\tLoss: 0.11854\n",
      "=> Epoch: 173\tLoss: 0.11873\n",
      "=> Epoch: 174\tLoss: 0.11869\n",
      "=> Epoch: 175\tLoss: 0.11797\n",
      "=> Epoch: 176\tLoss: 0.11778\n",
      "=> Epoch: 177\tLoss: 0.11707\n",
      "=> Epoch: 178\tLoss: 0.11716\n",
      "=> Epoch: 179\tLoss: 0.11870\n",
      "=> Epoch: 180\tLoss: 0.11710\n",
      "=> Epoch: 181\tLoss: 0.11863\n",
      "=> Epoch: 182\tLoss: 0.11857\n",
      "=> Epoch: 183\tLoss: 0.11858\n",
      "=> Epoch: 184\tLoss: 0.11859\n",
      "=> Epoch: 185\tLoss: 0.11836\n",
      "=> Epoch: 186\tLoss: 0.11777\n",
      "=> Epoch: 187\tLoss: 0.11718\n",
      "=> Epoch: 188\tLoss: 0.11782\n",
      "=> Epoch: 189\tLoss: 0.11868\n",
      "=> Epoch: 190\tLoss: 0.11787\n",
      "=> Epoch: 191\tLoss: 0.11784\n",
      "=> Epoch: 192\tLoss: 0.11707\n",
      "=> Epoch: 193\tLoss: 0.11848\n",
      "=> Epoch: 194\tLoss: 0.11785\n",
      "=> Epoch: 195\tLoss: 0.11855\n",
      "=> Epoch: 196\tLoss: 0.11784\n",
      "=> Epoch: 197\tLoss: 0.11864\n",
      "=> Epoch: 198\tLoss: 0.11856\n",
      "=> Epoch: 199\tLoss: 0.11779\n",
      "=> Epoch: 200\tLoss: 0.11858\n",
      "=> Epoch: 201\tLoss: 0.11959\n",
      "=> Epoch: 202\tLoss: 0.11776\n",
      "=> Epoch: 203\tLoss: 0.11779\n",
      "=> Epoch: 204\tLoss: 0.11775\n",
      "=> Epoch: 205\tLoss: 0.11779\n",
      "=> Epoch: 206\tLoss: 0.11849\n",
      "=> Epoch: 207\tLoss: 0.11784\n",
      "=> Epoch: 208\tLoss: 0.11854\n",
      "=> Epoch: 209\tLoss: 0.12002\n",
      "=> Epoch: 210\tLoss: 0.11942\n",
      "=> Epoch: 211\tLoss: 0.11720\n",
      "=> Epoch: 212\tLoss: 0.11836\n",
      "=> Epoch: 213\tLoss: 0.11836\n",
      "=> Epoch: 214\tLoss: 0.11705\n",
      "=> Epoch: 215\tLoss: 0.11784\n",
      "=> Epoch: 216\tLoss: 0.11717\n",
      "=> Epoch: 217\tLoss: 0.11955\n",
      "=> Epoch: 218\tLoss: 0.11858\n",
      "=> Epoch: 219\tLoss: 0.11772\n",
      "=> Epoch: 220\tLoss: 0.11858\n",
      "=> Epoch: 221\tLoss: 0.11845\n",
      "=> Epoch: 222\tLoss: 0.11789\n",
      "=> Epoch: 223\tLoss: 0.11861\n",
      "=> Epoch: 224\tLoss: 0.11848\n",
      "=> Epoch: 225\tLoss: 0.11772\n",
      "=> Epoch: 226\tLoss: 0.11846\n",
      "=> Epoch: 227\tLoss: 0.11775\n",
      "=> Epoch: 228\tLoss: 0.11845\n",
      "=> Epoch: 229\tLoss: 0.11716\n",
      "=> Epoch: 230\tLoss: 0.11846\n",
      "=> Epoch: 231\tLoss: 0.11856\n",
      "=> Epoch: 232\tLoss: 0.11716\n",
      "=> Epoch: 233\tLoss: 0.11853\n",
      "=> Epoch: 234\tLoss: 0.11781\n",
      "=> Epoch: 235\tLoss: 0.11857\n",
      "=> Epoch: 236\tLoss: 0.11774\n",
      "=> Epoch: 237\tLoss: 0.11992\n",
      "=> Epoch: 238\tLoss: 0.11856\n",
      "=> Epoch: 239\tLoss: 0.11882\n",
      "=> Epoch: 240\tLoss: 0.11707\n",
      "=> Epoch: 241\tLoss: 0.11845\n",
      "=> Epoch: 242\tLoss: 0.11769\n",
      "=> Epoch: 243\tLoss: 0.11798\n",
      "=> Epoch: 244\tLoss: 0.11772\n",
      "=> Epoch: 245\tLoss: 0.11777\n",
      "=> Epoch: 246\tLoss: 0.11853\n",
      "=> Epoch: 247\tLoss: 0.11761\n",
      "=> Epoch: 248\tLoss: 0.11852\n",
      "=> Epoch: 249\tLoss: 0.11698\n",
      "=> Epoch: 250\tLoss: 0.11783\n",
      "=> Epoch: 251\tLoss: 0.11847\n",
      "=> Epoch: 252\tLoss: 0.11851\n",
      "=> Epoch: 253\tLoss: 0.11710\n",
      "=> Epoch: 254\tLoss: 0.11844\n",
      "=> Epoch: 255\tLoss: 0.11859\n",
      "=> Epoch: 256\tLoss: 0.11852\n",
      "=> Epoch: 257\tLoss: 0.11706\n",
      "=> Epoch: 258\tLoss: 0.11796\n",
      "=> Epoch: 259\tLoss: 0.11852\n",
      "=> Epoch: 260\tLoss: 0.11847\n",
      "=> Epoch: 261\tLoss: 0.11785\n",
      "=> Epoch: 262\tLoss: 0.11847\n",
      "=> Epoch: 263\tLoss: 0.11835\n",
      "=> Epoch: 264\tLoss: 0.11774\n",
      "=> Epoch: 265\tLoss: 0.11785\n",
      "=> Epoch: 266\tLoss: 0.11844\n",
      "=> Epoch: 267\tLoss: 0.11847\n",
      "=> Epoch: 268\tLoss: 0.11710\n",
      "=> Epoch: 269\tLoss: 0.11858\n",
      "=> Epoch: 270\tLoss: 0.11784\n",
      "=> Epoch: 271\tLoss: 0.11759\n",
      "=> Epoch: 272\tLoss: 0.11846\n",
      "=> Epoch: 273\tLoss: 0.11783\n",
      "=> Epoch: 274\tLoss: 0.11835\n",
      "=> Epoch: 275\tLoss: 0.11779\n",
      "=> Epoch: 276\tLoss: 0.11853\n",
      "=> Epoch: 277\tLoss: 0.11774\n",
      "=> Epoch: 278\tLoss: 0.11847\n",
      "=> Epoch: 279\tLoss: 0.11831\n",
      "=> Epoch: 280\tLoss: 0.11777\n",
      "=> Epoch: 281\tLoss: 0.11771\n",
      "=> Epoch: 282\tLoss: 0.11848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Epoch: 283\tLoss: 0.11924\n",
      "=> Epoch: 284\tLoss: 0.11701\n",
      "=> Epoch: 285\tLoss: 0.11940\n",
      "=> Epoch: 286\tLoss: 0.11766\n",
      "=> Epoch: 287\tLoss: 0.11762\n",
      "=> Epoch: 288\tLoss: 0.11856\n",
      "=> Epoch: 289\tLoss: 0.11695\n",
      "=> Epoch: 290\tLoss: 0.11846\n",
      "=> Epoch: 291\tLoss: 0.11837\n",
      "=> Epoch: 292\tLoss: 0.11838\n",
      "=> Epoch: 293\tLoss: 0.11926\n",
      "=> Epoch: 294\tLoss: 0.11777\n",
      "=> Epoch: 295\tLoss: 0.12001\n",
      "=> Epoch: 296\tLoss: 0.11836\n",
      "=> Epoch: 297\tLoss: 0.11858\n",
      "=> Epoch: 298\tLoss: 0.11773\n",
      "=> Epoch: 299\tLoss: 0.11774\n",
      "=> Epoch: 300\tLoss: 0.11882\n",
      "=> Epoch: 301\tLoss: 0.11840\n",
      "=> Epoch: 302\tLoss: 0.11856\n",
      "=> Epoch: 303\tLoss: 0.11840\n",
      "=> Epoch: 304\tLoss: 0.11780\n",
      "=> Epoch: 305\tLoss: 0.11849\n",
      "=> Epoch: 306\tLoss: 0.11776\n",
      "=> Epoch: 307\tLoss: 0.11781\n",
      "=> Epoch: 308\tLoss: 0.11835\n",
      "=> Epoch: 309\tLoss: 0.11842\n",
      "=> Epoch: 310\tLoss: 0.11768\n",
      "=> Epoch: 311\tLoss: 0.11770\n",
      "=> Epoch: 312\tLoss: 0.11642\n",
      "=> Epoch: 313\tLoss: 0.11838\n",
      "=> Epoch: 314\tLoss: 0.11836\n",
      "=> Epoch: 315\tLoss: 0.11773\n",
      "=> Epoch: 316\tLoss: 0.11849\n",
      "=> Epoch: 317\tLoss: 0.11705\n",
      "=> Epoch: 318\tLoss: 0.11847\n",
      "=> Epoch: 319\tLoss: 0.11848\n",
      "=> Epoch: 320\tLoss: 0.11767\n",
      "=> Epoch: 321\tLoss: 0.11842\n",
      "=> Epoch: 322\tLoss: 0.11763\n",
      "=> Epoch: 323\tLoss: 0.11698\n",
      "=> Epoch: 324\tLoss: 0.11795\n",
      "=> Epoch: 325\tLoss: 0.11841\n",
      "=> Epoch: 326\tLoss: 0.11849\n",
      "=> Epoch: 327\tLoss: 0.11761\n",
      "=> Epoch: 328\tLoss: 0.11690\n",
      "=> Epoch: 329\tLoss: 0.11847\n",
      "=> Epoch: 330\tLoss: 0.11836\n",
      "=> Epoch: 331\tLoss: 0.11829\n",
      "=> Epoch: 332\tLoss: 0.11849\n",
      "=> Epoch: 333\tLoss: 0.11942\n",
      "=> Epoch: 334\tLoss: 0.11864\n",
      "=> Epoch: 335\tLoss: 0.11773\n",
      "=> Epoch: 336\tLoss: 0.11844\n",
      "=> Epoch: 337\tLoss: 0.11853\n",
      "=> Epoch: 338\tLoss: 0.11767\n",
      "=> Epoch: 339\tLoss: 0.11693\n",
      "=> Epoch: 340\tLoss: 0.11832\n",
      "=> Epoch: 341\tLoss: 0.11772\n",
      "=> Epoch: 342\tLoss: 0.11773\n",
      "=> Epoch: 343\tLoss: 0.11712\n",
      "=> Epoch: 344\tLoss: 0.11770\n",
      "=> Epoch: 345\tLoss: 0.11706\n",
      "=> Epoch: 346\tLoss: 0.11772\n",
      "=> Epoch: 347\tLoss: 0.11839\n",
      "=> Epoch: 348\tLoss: 0.11844\n",
      "=> Epoch: 349\tLoss: 0.11919\n",
      "=> Epoch: 350\tLoss: 0.11786\n",
      "=> Epoch: 351\tLoss: 0.11839\n",
      "=> Epoch: 352\tLoss: 0.11871\n",
      "=> Epoch: 353\tLoss: 0.11845\n",
      "=> Epoch: 354\tLoss: 0.11694\n",
      "=> Epoch: 355\tLoss: 0.11769\n",
      "=> Epoch: 356\tLoss: 0.11842\n",
      "=> Epoch: 357\tLoss: 0.11774\n",
      "=> Epoch: 358\tLoss: 0.11850\n",
      "=> Epoch: 359\tLoss: 0.11949\n",
      "=> Epoch: 360\tLoss: 0.11772\n",
      "=> Epoch: 361\tLoss: 0.11764\n",
      "=> Epoch: 362\tLoss: 0.11871\n",
      "=> Epoch: 363\tLoss: 0.11852\n",
      "=> Epoch: 364\tLoss: 0.11766\n",
      "=> Epoch: 365\tLoss: 0.11702\n",
      "=> Epoch: 366\tLoss: 0.11772\n",
      "=> Epoch: 367\tLoss: 0.11769\n",
      "=> Epoch: 368\tLoss: 0.11766\n",
      "=> Epoch: 369\tLoss: 0.11837\n",
      "=> Epoch: 370\tLoss: 0.11843\n",
      "=> Epoch: 371\tLoss: 0.11845\n",
      "=> Epoch: 372\tLoss: 0.11850\n",
      "=> Epoch: 373\tLoss: 0.11844\n",
      "=> Epoch: 374\tLoss: 0.11948\n",
      "=> Epoch: 375\tLoss: 0.11781\n",
      "=> Epoch: 376\tLoss: 0.11906\n",
      "=> Epoch: 377\tLoss: 0.11847\n",
      "=> Epoch: 378\tLoss: 0.11771\n",
      "=> Epoch: 379\tLoss: 0.11692\n",
      "=> Epoch: 380\tLoss: 0.11848\n",
      "=> Epoch: 381\tLoss: 0.11783\n",
      "=> Epoch: 382\tLoss: 0.11835\n",
      "=> Epoch: 383\tLoss: 0.11851\n",
      "=> Epoch: 384\tLoss: 0.11846\n",
      "=> Epoch: 385\tLoss: 0.11838\n",
      "=> Epoch: 386\tLoss: 0.11775\n",
      "=> Epoch: 387\tLoss: 0.11853\n",
      "=> Epoch: 388\tLoss: 0.11769\n",
      "=> Epoch: 389\tLoss: 0.11705\n",
      "=> Epoch: 390\tLoss: 0.11766\n",
      "=> Epoch: 391\tLoss: 0.11839\n",
      "=> Epoch: 392\tLoss: 0.11767\n",
      "=> Epoch: 393\tLoss: 0.11943\n",
      "=> Epoch: 394\tLoss: 0.11837\n",
      "=> Epoch: 395\tLoss: 0.11704\n",
      "=> Epoch: 396\tLoss: 0.11777\n",
      "=> Epoch: 397\tLoss: 0.11844\n",
      "=> Epoch: 398\tLoss: 0.11767\n",
      "=> Epoch: 399\tLoss: 0.11849\n",
      "=> Epoch: 400\tLoss: 0.11701\n",
      "=> Epoch: 401\tLoss: 0.11707\n",
      "=> Epoch: 402\tLoss: 0.11843\n",
      "=> Epoch: 403\tLoss: 0.11702\n",
      "=> Epoch: 404\tLoss: 0.11767\n",
      "=> Epoch: 405\tLoss: 0.11852\n",
      "=> Epoch: 406\tLoss: 0.11797\n",
      "=> Epoch: 407\tLoss: 0.11834\n",
      "=> Epoch: 408\tLoss: 0.11847\n",
      "=> Epoch: 409\tLoss: 0.11774\n",
      "=> Epoch: 410\tLoss: 0.11770\n",
      "=> Epoch: 411\tLoss: 0.11697\n",
      "=> Epoch: 412\tLoss: 0.11843\n",
      "=> Epoch: 413\tLoss: 0.11838\n",
      "=> Epoch: 414\tLoss: 0.11700\n",
      "=> Epoch: 415\tLoss: 0.11778\n",
      "=> Epoch: 416\tLoss: 0.11833\n",
      "=> Epoch: 417\tLoss: 0.11844\n",
      "=> Epoch: 418\tLoss: 0.11760\n",
      "=> Epoch: 419\tLoss: 0.11780\n",
      "=> Epoch: 420\tLoss: 0.11763\n",
      "=> Epoch: 421\tLoss: 0.11838\n",
      "=> Epoch: 422\tLoss: 0.11772\n",
      "=> Epoch: 423\tLoss: 0.11842\n",
      "=> Epoch: 424\tLoss: 0.11764\n",
      "=> Epoch: 425\tLoss: 0.11775\n",
      "=> Epoch: 426\tLoss: 0.11839\n",
      "=> Epoch: 427\tLoss: 0.11843\n",
      "=> Epoch: 428\tLoss: 0.11629\n",
      "=> Epoch: 429\tLoss: 0.11838\n",
      "=> Epoch: 430\tLoss: 0.11773\n",
      "=> Epoch: 431\tLoss: 0.11827\n",
      "=> Epoch: 432\tLoss: 0.11770\n",
      "=> Epoch: 433\tLoss: 0.11762\n",
      "=> Epoch: 434\tLoss: 0.11774\n",
      "=> Epoch: 435\tLoss: 0.11839\n",
      "=> Epoch: 436\tLoss: 0.11842\n",
      "=> Epoch: 437\tLoss: 0.11706\n",
      "=> Epoch: 438\tLoss: 0.11774\n",
      "=> Epoch: 439\tLoss: 0.11839\n",
      "=> Epoch: 440\tLoss: 0.11835\n",
      "=> Epoch: 441\tLoss: 0.11844\n",
      "=> Epoch: 442\tLoss: 0.11934\n",
      "=> Epoch: 443\tLoss: 0.11932\n",
      "=> Epoch: 444\tLoss: 0.11868\n",
      "=> Epoch: 445\tLoss: 0.11765\n",
      "=> Epoch: 446\tLoss: 0.11845\n",
      "=> Epoch: 447\tLoss: 0.11840\n",
      "=> Epoch: 448\tLoss: 0.11759\n",
      "=> Epoch: 449\tLoss: 0.11846\n",
      "=> Epoch: 450\tLoss: 0.11766\n",
      "=> Epoch: 451\tLoss: 0.11764\n",
      "=> Epoch: 452\tLoss: 0.11841\n",
      "=> Epoch: 453\tLoss: 0.11769\n",
      "=> Epoch: 454\tLoss: 0.11839\n",
      "=> Epoch: 455\tLoss: 0.11763\n",
      "=> Epoch: 456\tLoss: 0.11773\n",
      "=> Epoch: 457\tLoss: 0.11946\n",
      "=> Epoch: 458\tLoss: 0.11767\n",
      "=> Epoch: 459\tLoss: 0.11843\n",
      "=> Epoch: 460\tLoss: 0.11758\n",
      "=> Epoch: 461\tLoss: 0.11928\n",
      "=> Epoch: 462\tLoss: 0.11770\n",
      "=> Epoch: 463\tLoss: 0.11847\n",
      "=> Epoch: 464\tLoss: 0.11837\n",
      "=> Epoch: 465\tLoss: 0.11846\n",
      "=> Epoch: 466\tLoss: 0.11841\n",
      "=> Epoch: 467\tLoss: 0.11766\n",
      "=> Epoch: 468\tLoss: 0.11767\n",
      "=> Epoch: 469\tLoss: 0.11860\n",
      "=> Epoch: 470\tLoss: 0.11829\n",
      "=> Epoch: 471\tLoss: 0.11776\n",
      "=> Epoch: 472\tLoss: 0.11827\n",
      "=> Epoch: 473\tLoss: 0.11770\n",
      "=> Epoch: 474\tLoss: 0.11764\n",
      "=> Epoch: 475\tLoss: 0.11831\n",
      "=> Epoch: 476\tLoss: 0.11864\n",
      "=> Epoch: 477\tLoss: 0.11784\n",
      "=> Epoch: 478\tLoss: 0.11759\n",
      "=> Epoch: 479\tLoss: 0.11834\n",
      "=> Epoch: 480\tLoss: 0.12006\n",
      "=> Epoch: 481\tLoss: 0.11770\n",
      "=> Epoch: 482\tLoss: 0.11784\n",
      "=> Epoch: 483\tLoss: 0.11754\n",
      "=> Epoch: 484\tLoss: 0.11939\n",
      "=> Epoch: 485\tLoss: 0.11700\n",
      "=> Epoch: 486\tLoss: 0.11700\n",
      "=> Epoch: 487\tLoss: 0.11842\n",
      "=> Epoch: 488\tLoss: 0.11843\n",
      "=> Epoch: 489\tLoss: 0.11835\n",
      "=> Epoch: 490\tLoss: 0.11831\n",
      "=> Epoch: 491\tLoss: 0.11848\n",
      "=> Epoch: 492\tLoss: 0.11764\n",
      "=> Epoch: 493\tLoss: 0.11831\n",
      "=> Epoch: 494\tLoss: 0.11691\n",
      "=> Epoch: 495\tLoss: 0.11771\n",
      "=> Epoch: 496\tLoss: 0.11866\n",
      "=> Epoch: 497\tLoss: 0.11769\n",
      "=> Epoch: 498\tLoss: 0.11832\n",
      "=> Epoch: 499\tLoss: 0.11989\n",
      "=> Epoch: 500\tLoss: 0.11799\n",
      "=> Epoch: 501\tLoss: 0.11844\n",
      "=> Epoch: 502\tLoss: 0.11843\n",
      "=> Epoch: 503\tLoss: 0.11699\n",
      "=> Epoch: 504\tLoss: 0.11830\n",
      "=> Epoch: 505\tLoss: 0.11947\n",
      "=> Epoch: 506\tLoss: 0.11941\n",
      "=> Epoch: 507\tLoss: 0.11779\n",
      "=> Epoch: 508\tLoss: 0.11691\n",
      "=> Epoch: 509\tLoss: 0.11836\n",
      "=> Epoch: 510\tLoss: 0.11771\n",
      "=> Epoch: 511\tLoss: 0.11765\n",
      "=> Epoch: 512\tLoss: 0.11834\n",
      "=> Epoch: 513\tLoss: 0.11769\n",
      "=> Epoch: 514\tLoss: 0.11848\n",
      "=> Epoch: 515\tLoss: 0.11932\n",
      "=> Epoch: 516\tLoss: 0.11776\n",
      "=> Epoch: 517\tLoss: 0.11774\n",
      "=> Epoch: 518\tLoss: 0.11698\n",
      "=> Epoch: 519\tLoss: 0.11779\n",
      "=> Epoch: 520\tLoss: 0.11929\n",
      "=> Epoch: 521\tLoss: 0.11765\n",
      "=> Epoch: 522\tLoss: 0.11767\n",
      "=> Epoch: 523\tLoss: 0.11765\n",
      "=> Epoch: 524\tLoss: 0.11768\n",
      "=> Epoch: 525\tLoss: 0.11843\n",
      "=> Epoch: 526\tLoss: 0.11845\n",
      "=> Epoch: 527\tLoss: 0.11941\n",
      "=> Epoch: 528\tLoss: 0.11705\n",
      "=> Epoch: 529\tLoss: 0.11837\n",
      "=> Epoch: 530\tLoss: 0.11833\n",
      "=> Epoch: 531\tLoss: 0.11764\n",
      "=> Epoch: 532\tLoss: 0.11842\n",
      "=> Epoch: 533\tLoss: 0.11765\n",
      "=> Epoch: 534\tLoss: 0.11777\n",
      "=> Epoch: 535\tLoss: 0.11972\n",
      "=> Epoch: 536\tLoss: 0.11793\n",
      "=> Epoch: 537\tLoss: 0.11775\n",
      "=> Epoch: 538\tLoss: 0.11761\n",
      "=> Epoch: 539\tLoss: 0.11848\n",
      "=> Epoch: 540\tLoss: 0.11838\n",
      "=> Epoch: 541\tLoss: 0.11819\n",
      "=> Epoch: 542\tLoss: 0.11856\n",
      "=> Epoch: 543\tLoss: 0.11781\n",
      "=> Epoch: 544\tLoss: 0.11856\n",
      "=> Epoch: 545\tLoss: 0.11769\n",
      "=> Epoch: 546\tLoss: 0.11844\n",
      "=> Epoch: 547\tLoss: 0.11760\n",
      "=> Epoch: 548\tLoss: 0.11840\n",
      "=> Epoch: 549\tLoss: 0.11771\n",
      "=> Epoch: 550\tLoss: 0.11844\n",
      "=> Epoch: 551\tLoss: 0.11706\n",
      "=> Epoch: 552\tLoss: 0.11841\n",
      "=> Epoch: 553\tLoss: 0.11906\n",
      "=> Epoch: 554\tLoss: 0.11829\n",
      "=> Epoch: 555\tLoss: 0.11833\n",
      "=> Epoch: 556\tLoss: 0.11693\n",
      "=> Epoch: 557\tLoss: 0.11777\n",
      "=> Epoch: 558\tLoss: 0.11828\n",
      "=> Epoch: 559\tLoss: 0.11767\n",
      "=> Epoch: 560\tLoss: 0.11771\n",
      "=> Epoch: 561\tLoss: 0.11703\n",
      "=> Epoch: 562\tLoss: 0.11685\n",
      "=> Epoch: 563\tLoss: 0.11845\n",
      "=> Epoch: 564\tLoss: 0.11764\n",
      "=> Epoch: 565\tLoss: 0.11830\n",
      "=> Epoch: 566\tLoss: 0.11775\n",
      "=> Epoch: 567\tLoss: 0.11847\n",
      "=> Epoch: 568\tLoss: 0.11848\n",
      "=> Epoch: 569\tLoss: 0.11775\n",
      "=> Epoch: 570\tLoss: 0.11945\n",
      "=> Epoch: 571\tLoss: 0.11848\n",
      "=> Epoch: 572\tLoss: 0.11831\n",
      "=> Epoch: 573\tLoss: 0.11837\n",
      "=> Epoch: 574\tLoss: 0.11845\n",
      "=> Epoch: 575\tLoss: 0.11843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Epoch: 576\tLoss: 0.11688\n",
      "=> Epoch: 577\tLoss: 0.11842\n",
      "=> Epoch: 578\tLoss: 0.11841\n",
      "=> Epoch: 579\tLoss: 0.11944\n",
      "=> Epoch: 580\tLoss: 0.11923\n",
      "=> Epoch: 581\tLoss: 0.11710\n",
      "=> Epoch: 582\tLoss: 0.11692\n",
      "=> Epoch: 583\tLoss: 0.11840\n",
      "=> Epoch: 584\tLoss: 0.11764\n",
      "=> Epoch: 585\tLoss: 0.11695\n",
      "=> Epoch: 586\tLoss: 0.11839\n",
      "=> Epoch: 587\tLoss: 0.11768\n",
      "=> Epoch: 588\tLoss: 0.11767\n",
      "=> Epoch: 589\tLoss: 0.11758\n",
      "=> Epoch: 590\tLoss: 0.11770\n",
      "=> Epoch: 591\tLoss: 0.11635\n",
      "=> Epoch: 592\tLoss: 0.11945\n",
      "=> Epoch: 593\tLoss: 0.11839\n",
      "=> Epoch: 594\tLoss: 0.11831\n",
      "=> Epoch: 595\tLoss: 0.11691\n",
      "=> Epoch: 596\tLoss: 0.11840\n",
      "=> Epoch: 597\tLoss: 0.11702\n",
      "=> Epoch: 598\tLoss: 0.11851\n",
      "=> Epoch: 599\tLoss: 0.11932\n",
      "=> Epoch: 600\tLoss: 0.11859\n",
      "=> Epoch: 601\tLoss: 0.11836\n",
      "=> Epoch: 602\tLoss: 0.11820\n",
      "=> Epoch: 603\tLoss: 0.11776\n",
      "=> Epoch: 604\tLoss: 0.11771\n",
      "=> Epoch: 605\tLoss: 0.11843\n",
      "=> Epoch: 606\tLoss: 0.11932\n",
      "=> Epoch: 607\tLoss: 0.11829\n",
      "=> Epoch: 608\tLoss: 0.11763\n",
      "=> Epoch: 609\tLoss: 0.11699\n",
      "=> Epoch: 610\tLoss: 0.11842\n",
      "=> Epoch: 611\tLoss: 0.11776\n",
      "=> Epoch: 612\tLoss: 0.11633\n",
      "=> Epoch: 613\tLoss: 0.11844\n",
      "=> Epoch: 614\tLoss: 0.11688\n",
      "=> Epoch: 615\tLoss: 0.11846\n",
      "=> Epoch: 616\tLoss: 0.11835\n",
      "=> Epoch: 617\tLoss: 0.11767\n",
      "=> Epoch: 618\tLoss: 0.11845\n",
      "=> Epoch: 619\tLoss: 0.11836\n",
      "=> Epoch: 620\tLoss: 0.11775\n",
      "=> Epoch: 621\tLoss: 0.11770\n",
      "=> Epoch: 622\tLoss: 0.11691\n",
      "=> Epoch: 623\tLoss: 0.11626\n",
      "=> Epoch: 624\tLoss: 0.11832\n",
      "=> Epoch: 625\tLoss: 0.11841\n",
      "=> Epoch: 626\tLoss: 0.11835\n",
      "=> Epoch: 627\tLoss: 0.11832\n",
      "=> Epoch: 628\tLoss: 0.11767\n",
      "=> Epoch: 629\tLoss: 0.11766\n",
      "=> Epoch: 630\tLoss: 0.11839\n",
      "=> Epoch: 631\tLoss: 0.11844\n",
      "=> Epoch: 632\tLoss: 0.11622\n",
      "=> Epoch: 633\tLoss: 0.11769\n",
      "=> Epoch: 634\tLoss: 0.11623\n",
      "=> Epoch: 635\tLoss: 0.11831\n",
      "=> Epoch: 636\tLoss: 0.11767\n",
      "=> Epoch: 637\tLoss: 0.11702\n",
      "=> Epoch: 638\tLoss: 0.11770\n",
      "=> Epoch: 639\tLoss: 0.11771\n",
      "=> Epoch: 640\tLoss: 0.11988\n",
      "=> Epoch: 641\tLoss: 0.11842\n",
      "=> Epoch: 642\tLoss: 0.11857\n",
      "=> Epoch: 643\tLoss: 0.11946\n",
      "=> Epoch: 644\tLoss: 0.11852\n",
      "=> Epoch: 645\tLoss: 0.11694\n",
      "=> Epoch: 646\tLoss: 0.11773\n",
      "=> Epoch: 647\tLoss: 0.12020\n",
      "=> Epoch: 648\tLoss: 0.11762\n",
      "=> Epoch: 649\tLoss: 0.11840\n",
      "=> Epoch: 650\tLoss: 0.11835\n",
      "=> Epoch: 651\tLoss: 0.11837\n",
      "=> Epoch: 652\tLoss: 0.11828\n",
      "=> Epoch: 653\tLoss: 0.11831\n",
      "=> Epoch: 654\tLoss: 0.11828\n",
      "=> Epoch: 655\tLoss: 0.11762\n",
      "=> Epoch: 656\tLoss: 0.11760\n",
      "=> Epoch: 657\tLoss: 0.11761\n",
      "=> Epoch: 658\tLoss: 0.11839\n",
      "=> Epoch: 659\tLoss: 0.11924\n",
      "=> Epoch: 660\tLoss: 0.11752\n",
      "=> Epoch: 661\tLoss: 0.11759\n",
      "=> Epoch: 662\tLoss: 0.11758\n",
      "=> Epoch: 663\tLoss: 0.11849\n",
      "=> Epoch: 664\tLoss: 0.11754\n",
      "=> Epoch: 665\tLoss: 0.11702\n",
      "=> Epoch: 666\tLoss: 0.11759\n",
      "=> Epoch: 667\tLoss: 0.11851\n",
      "=> Epoch: 668\tLoss: 0.11696\n",
      "=> Epoch: 669\tLoss: 0.11774\n",
      "=> Epoch: 670\tLoss: 0.11843\n",
      "=> Epoch: 671\tLoss: 0.11764\n",
      "=> Epoch: 672\tLoss: 0.11837\n",
      "=> Epoch: 673\tLoss: 0.11835\n",
      "=> Epoch: 674\tLoss: 0.11840\n",
      "=> Epoch: 675\tLoss: 0.11760\n",
      "=> Epoch: 676\tLoss: 0.11843\n",
      "=> Epoch: 677\tLoss: 0.11842\n",
      "=> Epoch: 678\tLoss: 0.11836\n",
      "=> Epoch: 679\tLoss: 0.11858\n",
      "=> Epoch: 680\tLoss: 0.11765\n",
      "=> Epoch: 681\tLoss: 0.11771\n",
      "=> Epoch: 682\tLoss: 0.11829\n",
      "=> Epoch: 683\tLoss: 0.11835\n",
      "=> Epoch: 684\tLoss: 0.11841\n",
      "=> Epoch: 685\tLoss: 0.11765\n",
      "=> Epoch: 686\tLoss: 0.11765\n",
      "=> Epoch: 687\tLoss: 0.11867\n",
      "=> Epoch: 688\tLoss: 0.11695\n",
      "=> Epoch: 689\tLoss: 0.11839\n",
      "=> Epoch: 690\tLoss: 0.11834\n",
      "=> Epoch: 691\tLoss: 0.11876\n",
      "=> Epoch: 692\tLoss: 0.11775\n",
      "=> Epoch: 693\tLoss: 0.11848\n",
      "=> Epoch: 694\tLoss: 0.11691\n",
      "=> Epoch: 695\tLoss: 0.11840\n",
      "=> Epoch: 696\tLoss: 0.11839\n",
      "=> Epoch: 697\tLoss: 0.11851\n",
      "=> Epoch: 698\tLoss: 0.11829\n",
      "=> Epoch: 699\tLoss: 0.11838\n",
      "=> Epoch: 700\tLoss: 0.11838\n",
      "=> Epoch: 701\tLoss: 0.11898\n",
      "=> Epoch: 702\tLoss: 0.11842\n",
      "=> Epoch: 703\tLoss: 0.11767\n",
      "=> Epoch: 704\tLoss: 0.11947\n",
      "=> Epoch: 705\tLoss: 0.11755\n",
      "=> Epoch: 706\tLoss: 0.11772\n",
      "=> Epoch: 707\tLoss: 0.11699\n",
      "=> Epoch: 708\tLoss: 0.11704\n",
      "=> Epoch: 709\tLoss: 0.11702\n",
      "=> Epoch: 710\tLoss: 0.11770\n",
      "=> Epoch: 711\tLoss: 0.11826\n",
      "=> Epoch: 712\tLoss: 0.11840\n",
      "=> Epoch: 713\tLoss: 0.11696\n",
      "=> Epoch: 714\tLoss: 0.11759\n",
      "=> Epoch: 715\tLoss: 0.11764\n",
      "=> Epoch: 716\tLoss: 0.11775\n",
      "=> Epoch: 717\tLoss: 0.11834\n",
      "=> Epoch: 718\tLoss: 0.11750\n",
      "=> Epoch: 719\tLoss: 0.11692\n",
      "=> Epoch: 720\tLoss: 0.11835\n",
      "=> Epoch: 721\tLoss: 0.11690\n",
      "=> Epoch: 722\tLoss: 0.11850\n",
      "=> Epoch: 723\tLoss: 0.11762\n",
      "=> Epoch: 724\tLoss: 0.11874\n",
      "=> Epoch: 725\tLoss: 0.11889\n",
      "=> Epoch: 726\tLoss: 0.11781\n",
      "=> Epoch: 727\tLoss: 0.11825\n",
      "=> Epoch: 728\tLoss: 0.11764\n",
      "=> Epoch: 729\tLoss: 0.11859\n",
      "=> Epoch: 730\tLoss: 0.11840\n",
      "=> Epoch: 731\tLoss: 0.11840\n",
      "=> Epoch: 732\tLoss: 0.11695\n",
      "=> Epoch: 733\tLoss: 0.11827\n",
      "=> Epoch: 734\tLoss: 0.11784\n",
      "=> Epoch: 735\tLoss: 0.11946\n",
      "=> Epoch: 736\tLoss: 0.11767\n",
      "=> Epoch: 737\tLoss: 0.11769\n",
      "=> Epoch: 738\tLoss: 0.11831\n",
      "=> Epoch: 739\tLoss: 0.11833\n",
      "=> Epoch: 740\tLoss: 0.11772\n",
      "=> Epoch: 741\tLoss: 0.11829\n",
      "=> Epoch: 742\tLoss: 0.11952\n",
      "=> Epoch: 743\tLoss: 0.11848\n",
      "=> Epoch: 744\tLoss: 0.11764\n",
      "=> Epoch: 745\tLoss: 0.11863\n",
      "=> Epoch: 746\tLoss: 0.11839\n",
      "=> Epoch: 747\tLoss: 0.11834\n",
      "=> Epoch: 748\tLoss: 0.11927\n",
      "=> Epoch: 749\tLoss: 0.11831\n",
      "=> Epoch: 750\tLoss: 0.11841\n",
      "=> Epoch: 751\tLoss: 0.11620\n",
      "=> Epoch: 752\tLoss: 0.11768\n",
      "=> Epoch: 753\tLoss: 0.11839\n",
      "=> Epoch: 754\tLoss: 0.11752\n",
      "=> Epoch: 755\tLoss: 0.11768\n",
      "=> Epoch: 756\tLoss: 0.11766\n",
      "=> Epoch: 757\tLoss: 0.11697\n",
      "=> Epoch: 758\tLoss: 0.11765\n",
      "=> Epoch: 759\tLoss: 0.11690\n",
      "=> Epoch: 760\tLoss: 0.11935\n",
      "=> Epoch: 761\tLoss: 0.11791\n",
      "=> Epoch: 762\tLoss: 0.11837\n",
      "=> Epoch: 763\tLoss: 0.11765\n",
      "=> Epoch: 764\tLoss: 0.11748\n",
      "=> Epoch: 765\tLoss: 0.11844\n",
      "=> Epoch: 766\tLoss: 0.11841\n",
      "=> Epoch: 767\tLoss: 0.11831\n",
      "=> Epoch: 768\tLoss: 0.11914\n",
      "=> Epoch: 769\tLoss: 0.11765\n",
      "=> Epoch: 770\tLoss: 0.11763\n",
      "=> Epoch: 771\tLoss: 0.11766\n",
      "=> Epoch: 772\tLoss: 0.11760\n",
      "=> Epoch: 773\tLoss: 0.11834\n",
      "=> Epoch: 774\tLoss: 0.11767\n",
      "=> Epoch: 775\tLoss: 0.11764\n",
      "=> Epoch: 776\tLoss: 0.11843\n",
      "=> Epoch: 777\tLoss: 0.11703\n",
      "=> Epoch: 778\tLoss: 0.11694\n",
      "=> Epoch: 779\tLoss: 0.11763\n",
      "=> Epoch: 780\tLoss: 0.11834\n",
      "=> Epoch: 781\tLoss: 0.11831\n",
      "=> Epoch: 782\tLoss: 0.11767\n",
      "=> Epoch: 783\tLoss: 0.11767\n",
      "=> Epoch: 784\tLoss: 0.12025\n",
      "=> Epoch: 785\tLoss: 0.11841\n",
      "=> Epoch: 786\tLoss: 0.11762\n",
      "=> Epoch: 787\tLoss: 0.11754\n",
      "=> Epoch: 788\tLoss: 0.11760\n",
      "=> Epoch: 789\tLoss: 0.11841\n",
      "=> Epoch: 790\tLoss: 0.11757\n",
      "=> Epoch: 791\tLoss: 0.11768\n",
      "=> Epoch: 792\tLoss: 0.11771\n",
      "=> Epoch: 793\tLoss: 0.11688\n",
      "=> Epoch: 794\tLoss: 0.11839\n",
      "=> Epoch: 795\tLoss: 0.11779\n",
      "=> Epoch: 796\tLoss: 0.11832\n",
      "=> Epoch: 797\tLoss: 0.11823\n",
      "=> Epoch: 798\tLoss: 0.11751\n",
      "=> Epoch: 799\tLoss: 0.11768\n",
      "=> Epoch: 800\tLoss: 0.11934\n",
      "=> Epoch: 801\tLoss: 0.11754\n",
      "=> Epoch: 802\tLoss: 0.11848\n",
      "=> Epoch: 803\tLoss: 0.11769\n",
      "=> Epoch: 804\tLoss: 0.11764\n",
      "=> Epoch: 805\tLoss: 0.11755\n",
      "=> Epoch: 806\tLoss: 0.11628\n",
      "=> Epoch: 807\tLoss: 0.11768\n",
      "=> Epoch: 808\tLoss: 0.11770\n",
      "=> Epoch: 809\tLoss: 0.11853\n",
      "=> Epoch: 810\tLoss: 0.11837\n",
      "=> Epoch: 811\tLoss: 0.11840\n",
      "=> Epoch: 812\tLoss: 0.11685\n",
      "=> Epoch: 813\tLoss: 0.11688\n",
      "=> Epoch: 814\tLoss: 0.11773\n",
      "=> Epoch: 815\tLoss: 0.11762\n",
      "=> Epoch: 816\tLoss: 0.11828\n",
      "=> Epoch: 817\tLoss: 0.11836\n",
      "=> Epoch: 818\tLoss: 0.11766\n",
      "=> Epoch: 819\tLoss: 0.11704\n",
      "=> Epoch: 820\tLoss: 0.11690\n",
      "=> Epoch: 821\tLoss: 0.11773\n",
      "=> Epoch: 822\tLoss: 0.11841\n",
      "=> Epoch: 823\tLoss: 0.11834\n",
      "=> Epoch: 824\tLoss: 0.11926\n",
      "=> Epoch: 825\tLoss: 0.11845\n",
      "=> Epoch: 826\tLoss: 0.11829\n",
      "=> Epoch: 827\tLoss: 0.11838\n",
      "=> Epoch: 828\tLoss: 0.11837\n",
      "=> Epoch: 829\tLoss: 0.11696\n",
      "=> Epoch: 830\tLoss: 0.11764\n",
      "=> Epoch: 831\tLoss: 0.11921\n",
      "=> Epoch: 832\tLoss: 0.11846\n",
      "=> Epoch: 833\tLoss: 0.12014\n",
      "=> Epoch: 834\tLoss: 0.11693\n",
      "=> Epoch: 835\tLoss: 0.11825\n",
      "=> Epoch: 836\tLoss: 0.11756\n",
      "=> Epoch: 837\tLoss: 0.11839\n",
      "=> Epoch: 838\tLoss: 0.11757\n",
      "=> Epoch: 839\tLoss: 0.11762\n",
      "=> Epoch: 840\tLoss: 0.11831\n",
      "=> Epoch: 841\tLoss: 0.11766\n",
      "=> Epoch: 842\tLoss: 0.11836\n",
      "=> Epoch: 843\tLoss: 0.11830\n",
      "=> Epoch: 844\tLoss: 0.11833\n",
      "=> Epoch: 845\tLoss: 0.11835\n",
      "=> Epoch: 846\tLoss: 0.11832\n",
      "=> Epoch: 847\tLoss: 0.11837\n",
      "=> Epoch: 848\tLoss: 0.11754\n",
      "=> Epoch: 849\tLoss: 0.11822\n",
      "=> Epoch: 850\tLoss: 0.11762\n",
      "=> Epoch: 851\tLoss: 0.11839\n",
      "=> Epoch: 852\tLoss: 0.11692\n",
      "=> Epoch: 853\tLoss: 0.11829\n",
      "=> Epoch: 854\tLoss: 0.11767\n",
      "=> Epoch: 855\tLoss: 0.11755\n",
      "=> Epoch: 856\tLoss: 0.11682\n",
      "=> Epoch: 857\tLoss: 0.11769\n",
      "=> Epoch: 858\tLoss: 0.11555\n",
      "=> Epoch: 859\tLoss: 0.11686\n",
      "=> Epoch: 860\tLoss: 0.11753\n",
      "=> Epoch: 861\tLoss: 0.11774\n",
      "=> Epoch: 862\tLoss: 0.11764\n",
      "=> Epoch: 863\tLoss: 0.11686\n",
      "=> Epoch: 864\tLoss: 0.11836\n",
      "=> Epoch: 865\tLoss: 0.11689\n",
      "=> Epoch: 866\tLoss: 0.11915\n",
      "=> Epoch: 867\tLoss: 0.11772\n",
      "=> Epoch: 868\tLoss: 0.11757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Epoch: 869\tLoss: 0.11769\n",
      "=> Epoch: 870\tLoss: 0.11842\n",
      "=> Epoch: 871\tLoss: 0.11836\n",
      "=> Epoch: 872\tLoss: 0.11828\n",
      "=> Epoch: 873\tLoss: 0.11757\n",
      "=> Epoch: 874\tLoss: 0.11833\n",
      "=> Epoch: 875\tLoss: 0.11830\n",
      "=> Epoch: 876\tLoss: 0.11826\n",
      "=> Epoch: 877\tLoss: 0.11767\n",
      "=> Epoch: 878\tLoss: 0.11815\n",
      "=> Epoch: 879\tLoss: 0.11757\n",
      "=> Epoch: 880\tLoss: 0.11757\n",
      "=> Epoch: 881\tLoss: 0.11616\n",
      "=> Epoch: 882\tLoss: 0.11835\n",
      "=> Epoch: 883\tLoss: 0.11849\n",
      "=> Epoch: 884\tLoss: 0.11836\n",
      "=> Epoch: 885\tLoss: 0.11837\n",
      "=> Epoch: 886\tLoss: 0.11759\n",
      "=> Epoch: 887\tLoss: 0.11839\n",
      "=> Epoch: 888\tLoss: 0.11840\n",
      "=> Epoch: 889\tLoss: 0.11829\n",
      "=> Epoch: 890\tLoss: 0.11769\n",
      "=> Epoch: 891\tLoss: 0.11830\n",
      "=> Epoch: 892\tLoss: 0.11686\n",
      "=> Epoch: 893\tLoss: 0.11846\n",
      "=> Epoch: 894\tLoss: 0.11931\n",
      "=> Epoch: 895\tLoss: 0.11848\n",
      "=> Epoch: 896\tLoss: 0.11761\n",
      "=> Epoch: 897\tLoss: 0.11851\n",
      "=> Epoch: 898\tLoss: 0.11760\n",
      "=> Epoch: 899\tLoss: 0.11764\n",
      "=> Epoch: 900\tLoss: 0.11834\n",
      "=> Epoch: 901\tLoss: 0.11930\n",
      "=> Epoch: 902\tLoss: 0.11763\n",
      "=> Epoch: 903\tLoss: 0.11927\n",
      "=> Epoch: 904\tLoss: 0.11836\n",
      "=> Epoch: 905\tLoss: 0.11697\n",
      "=> Epoch: 906\tLoss: 0.11766\n",
      "=> Epoch: 907\tLoss: 0.11825\n",
      "=> Epoch: 908\tLoss: 0.11824\n",
      "=> Epoch: 909\tLoss: 0.11775\n",
      "=> Epoch: 910\tLoss: 0.11842\n",
      "=> Epoch: 911\tLoss: 0.11771\n",
      "=> Epoch: 912\tLoss: 0.11828\n",
      "=> Epoch: 913\tLoss: 0.11772\n",
      "=> Epoch: 914\tLoss: 0.11828\n",
      "=> Epoch: 915\tLoss: 0.11927\n",
      "=> Epoch: 916\tLoss: 0.11828\n",
      "=> Epoch: 917\tLoss: 0.11771\n",
      "=> Epoch: 918\tLoss: 0.11767\n",
      "=> Epoch: 919\tLoss: 0.11835\n",
      "=> Epoch: 920\tLoss: 0.11973\n",
      "=> Epoch: 921\tLoss: 0.11698\n",
      "=> Epoch: 922\tLoss: 0.11762\n",
      "=> Epoch: 923\tLoss: 0.11849\n",
      "=> Epoch: 924\tLoss: 0.11825\n",
      "=> Epoch: 925\tLoss: 0.11756\n",
      "=> Epoch: 926\tLoss: 0.11833\n",
      "=> Epoch: 927\tLoss: 0.11826\n",
      "=> Epoch: 928\tLoss: 0.11829\n",
      "=> Epoch: 929\tLoss: 0.11826\n",
      "=> Epoch: 930\tLoss: 0.11768\n",
      "=> Epoch: 931\tLoss: 0.11755\n",
      "=> Epoch: 932\tLoss: 0.11765\n",
      "=> Epoch: 933\tLoss: 0.11771\n",
      "=> Epoch: 934\tLoss: 0.11751\n",
      "=> Epoch: 935\tLoss: 0.11928\n",
      "=> Epoch: 936\tLoss: 0.11768\n",
      "=> Epoch: 937\tLoss: 0.11753\n",
      "=> Epoch: 938\tLoss: 0.11835\n",
      "=> Epoch: 939\tLoss: 0.11827\n",
      "=> Epoch: 940\tLoss: 0.11831\n",
      "=> Epoch: 941\tLoss: 0.11932\n",
      "=> Epoch: 942\tLoss: 0.11756\n",
      "=> Epoch: 943\tLoss: 0.11776\n",
      "=> Epoch: 944\tLoss: 0.11756\n",
      "=> Epoch: 945\tLoss: 0.11831\n",
      "=> Epoch: 946\tLoss: 0.11757\n",
      "=> Epoch: 947\tLoss: 0.11762\n",
      "=> Epoch: 948\tLoss: 0.11830\n",
      "=> Epoch: 949\tLoss: 0.11833\n",
      "=> Epoch: 950\tLoss: 0.11746\n",
      "=> Epoch: 951\tLoss: 0.11833\n",
      "=> Epoch: 952\tLoss: 0.11843\n",
      "=> Epoch: 953\tLoss: 0.11683\n",
      "=> Epoch: 954\tLoss: 0.11833\n",
      "=> Epoch: 955\tLoss: 0.11840\n",
      "=> Epoch: 956\tLoss: 0.11829\n",
      "=> Epoch: 957\tLoss: 0.11777\n",
      "=> Epoch: 958\tLoss: 0.11829\n",
      "=> Epoch: 959\tLoss: 0.11771\n",
      "=> Epoch: 960\tLoss: 0.11749\n",
      "=> Epoch: 961\tLoss: 0.11827\n",
      "=> Epoch: 962\tLoss: 0.11686\n",
      "=> Epoch: 963\tLoss: 0.11761\n",
      "=> Epoch: 964\tLoss: 0.11828\n",
      "=> Epoch: 965\tLoss: 0.11858\n",
      "=> Epoch: 966\tLoss: 0.11830\n",
      "=> Epoch: 967\tLoss: 0.11907\n",
      "=> Epoch: 968\tLoss: 0.11782\n",
      "=> Epoch: 969\tLoss: 0.11697\n",
      "=> Epoch: 970\tLoss: 0.11839\n",
      "=> Epoch: 971\tLoss: 0.11775\n",
      "=> Epoch: 972\tLoss: 0.11827\n",
      "=> Epoch: 973\tLoss: 0.11835\n",
      "=> Epoch: 974\tLoss: 0.11846\n",
      "=> Epoch: 975\tLoss: 0.11837\n",
      "=> Epoch: 976\tLoss: 0.11761\n",
      "=> Epoch: 977\tLoss: 0.11763\n",
      "=> Epoch: 978\tLoss: 0.11751\n",
      "=> Epoch: 979\tLoss: 0.11825\n",
      "=> Epoch: 980\tLoss: 0.11774\n",
      "=> Epoch: 981\tLoss: 0.11831\n",
      "=> Epoch: 982\tLoss: 0.11829\n",
      "=> Epoch: 983\tLoss: 0.11824\n",
      "=> Epoch: 984\tLoss: 0.11691\n",
      "=> Epoch: 985\tLoss: 0.11767\n",
      "=> Epoch: 986\tLoss: 0.11920\n",
      "=> Epoch: 987\tLoss: 0.11829\n",
      "=> Epoch: 988\tLoss: 0.11759\n",
      "=> Epoch: 989\tLoss: 0.11835\n",
      "=> Epoch: 990\tLoss: 0.11825\n",
      "=> Epoch: 991\tLoss: 0.11828\n",
      "=> Epoch: 992\tLoss: 0.11753\n",
      "=> Epoch: 993\tLoss: 0.11852\n",
      "=> Epoch: 994\tLoss: 0.11756\n",
      "=> Epoch: 995\tLoss: 0.11757\n",
      "=> Epoch: 996\tLoss: 0.11759\n",
      "=> Epoch: 997\tLoss: 0.11944\n",
      "=> Epoch: 998\tLoss: 0.11752\n",
      "=> Epoch: 999\tLoss: 0.11759\n",
      "=> Epoch: 1000\tLoss: 0.11922\n",
      "Setting optimal threshold...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 16875]' is invalid for input of size 31620000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-16996a5338e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCnnAutoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannel_maps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/workspace/pyntrainer/notebooks/../pyntrainer/lib/cnn_autoencoder.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, epochs, lr, batch_size, with_thresholding)\u001b[0m\n\u001b[1;32m    147\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Setting optimal threshold...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_optimal_threshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/workspace/pyntrainer/notebooks/../pyntrainer/lib/cnn_autoencoder.py\u001b[0m in \u001b[0;36mset_optimal_threshold\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mset_optimal_threshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;31m# Calculate the number of bins according to Freedman-Diaconis rule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/pyntrainer/notebooks/../pyntrainer/lib/cnn_autoencoder.py\u001b[0m in \u001b[0;36merrors\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'none'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_channels\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_width\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_channels\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_width\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 16875]' is invalid for input of size 31620000"
     ]
    }
   ],
   "source": [
    "net = CnnAutoencoder(channel_maps=[3,16,8], img_width=img_width, img_height=img_height)\n",
    "print(net)\n",
    "net.fit(torch.tensor(x), lr=0.005, batch_size=50, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.errors(torch.tensor(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = net.forward(torch.tensor(x))\n",
    "\n",
    "err = nn.BCELoss(reduction='none')(y.view(-1, 3 * 100 * 100), torch.tensor(x).view(-1, 3 * 100 * 100)).mean(axis=1)\n",
    "err\n",
    "\n",
    "#for y_n in y:\n",
    "#    plt.imshow(y_n[0].detach(), cmap='gray')\n",
    "#    plt.show()\n",
    "#len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.predict(torch.tensor(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.optimal_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_images_from_folder(\"/home/ralampay/Desktop/train\")\n",
    "\n",
    "# Perform a transpose against numpy to be valid shape for tensor\n",
    "\n",
    "x = []\n",
    "\n",
    "for img in images:\n",
    "    img_tensor = torch.tensor(img.transpose((2,0,1))).float()\n",
    "    d = img_tensor.detach().numpy()\n",
    "    x.append(d)\n",
    "\n",
    "y = net.forward(torch.tensor(x))\n",
    "for y_n in y:\n",
    "    plt.imshow(y_n[0].detach(), cmap='gray')\n",
    "    plt.show()\n",
    "    \n",
    "y = net.predict(torch.tensor(x))\n",
    "print(y[y == -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
